{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd055e70fe011e9c113bd50bf6016a6572c9ea2627b0654d216fef3044bd17027cd",
   "display_name": "Python 3.8.3  ('env': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "55e70fe011e9c113bd50bf6016a6572c9ea2627b0654d216fef3044bd17027cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "The purpose of this notebook is to get the head position and direction of the ferret for each stimulus presentation.\n",
    "\n",
    "The input data consists of two sources:\n",
    "(1) Stimulus metadata after temporal alignment to both video and neural recording systems\n",
    "(2) LED tracking data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(829, 10)\n(18345, 14)\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path('/home/stephen/Github/Speaker_Grid/data/F1901_Crumble_Squid/2021-05-27_Squid_15-57')\n",
    "\n",
    "stim_file = '2021-05-27T17-57-07_StimData_MCSVidAlign.csv'\n",
    "tracking_file = '2021-05-27_SquidVid_17_56_08.csv'\n",
    "\n",
    "stim = pd.read_csv( str(data_dir / stim_file))\n",
    "\n",
    "print(stim.shape)\n",
    "\n",
    "LEDs = pd.read_csv( str(data_dir / tracking_file))\n",
    "\n",
    "print(LEDs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(829, 14)\n"
     ]
    }
   ],
   "source": [
    "join_data = pd.merge( stim, LEDs[['blue_x','red_x','blue_y','red_y']], left_on='closest_frame', right_index=True)\n",
    "\n",
    "print(join_data.shape)"
   ]
  },
  {
   "source": [
    "### Tracking data: Problematic positions\n",
    "\n",
    "We know that when the LEDs go out of view, the tracking code returns ridiculous values. Here we replace anything that isn't sensible with nans.\n",
    "\n",
    "Note that y values tend to be reasonable even when x values are not (e.g. x=2703, y=123, for a 640x480 image) and so we stop the calculation using the nan on the x value."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_data.loc[join_data['blue_x'] < 150, 'blue_x'] = np.nan\n",
    "join_data.loc[join_data['blue_x'] > 500, 'blue_x'] = np.nan\n",
    "join_data.loc[join_data['blue_y'] < 0, 'blue_y'] = np.nan\n",
    "join_data.loc[join_data['blue_y'] > 480, 'blue_y'] = np.nan\n",
    "\n",
    "join_data.loc[join_data['red_x'] < 150, 'red_x'] = np.nan\n",
    "join_data.loc[join_data['red_x'] > 500, 'red_x'] = np.nan\n",
    "join_data.loc[join_data['red_y'] < 0, 'red_x'] = np.nan\n",
    "join_data.loc[join_data['red_y'] > 480, 'red_x'] = np.nan\n"
   ]
  },
  {
   "source": [
    "Calculate head position as center between red and blue LEDs - this is limited because of the poor detection of the red LED and could be improved using only the signal from the blue LED, but since we care most about head direction and need both LEDs to calculate head angle, we'll stick with it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       head_x      blue_x       red_x      head_y      blue_y       red_y\n",
       "0  323.310369  317.981450  328.639288  108.732587  107.364295  110.100879\n",
       "1  324.682402  318.601083  330.763721  106.546165  104.032659  109.059672\n",
       "2  324.526259  318.309560  330.742959  107.277053  105.433078  109.121029\n",
       "3  324.755314  318.678372  330.832257  106.759202  104.321904  109.196499\n",
       "4         NaN  317.878278         NaN  193.557949  107.095400  280.020499"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>head_x</th>\n      <th>blue_x</th>\n      <th>red_x</th>\n      <th>head_y</th>\n      <th>blue_y</th>\n      <th>red_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>323.310369</td>\n      <td>317.981450</td>\n      <td>328.639288</td>\n      <td>108.732587</td>\n      <td>107.364295</td>\n      <td>110.100879</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>324.682402</td>\n      <td>318.601083</td>\n      <td>330.763721</td>\n      <td>106.546165</td>\n      <td>104.032659</td>\n      <td>109.059672</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>324.526259</td>\n      <td>318.309560</td>\n      <td>330.742959</td>\n      <td>107.277053</td>\n      <td>105.433078</td>\n      <td>109.121029</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>324.755314</td>\n      <td>318.678372</td>\n      <td>330.832257</td>\n      <td>106.759202</td>\n      <td>104.321904</td>\n      <td>109.196499</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>317.878278</td>\n      <td>NaN</td>\n      <td>193.557949</td>\n      <td>107.095400</td>\n      <td>280.020499</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "join_data['head_x'] = (join_data['blue_x'] + join_data['red_x']) / 2\n",
    "join_data['head_y'] = (join_data['blue_y'] + join_data['red_y']) / 2\n",
    "\n",
    "# Show some examples to get a feel for the data\n",
    "join_data[['head_x','blue_x','red_x','head_y','blue_y','red_y']].head(n=5)\n"
   ]
  },
  {
   "source": [
    "Get head direction - here we're assuming that the red LED is on the right side of the head, and the blue LED is on the left side, such that the vector between them represents that interaural axis. In reality, there is a slight rotation in the vector that offsets it from the interaural axis, but we're ignoring that to develop the analysis quickly (and the 2D solution to this problem is inherantly bad anyway)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   head_angle  blue_zero_x  blue_zero_y\n",
       "0   -2.890255    -5.328919    -1.368292\n",
       "1   -2.749660    -6.081319    -2.513506\n",
       "2   -2.853243    -6.216700    -1.843976\n",
       "3   -2.760162    -6.076943    -2.437297\n",
       "4         NaN          NaN   -86.462549"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>head_angle</th>\n      <th>blue_zero_x</th>\n      <th>blue_zero_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-2.890255</td>\n      <td>-5.328919</td>\n      <td>-1.368292</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-2.749660</td>\n      <td>-6.081319</td>\n      <td>-2.513506</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-2.853243</td>\n      <td>-6.216700</td>\n      <td>-1.843976</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-2.760162</td>\n      <td>-6.076943</td>\n      <td>-2.437297</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-86.462549</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "join_data['blue_zero_x'] = join_data['blue_x'] - join_data['head_x']\n",
    "join_data['blue_zero_y'] = join_data['blue_y'] - join_data['head_y']\n",
    "\n",
    "join_data['head_angle'] = np.arctan2(join_data['blue_zero_y'], join_data['blue_zero_x'])\n",
    "\n",
    "join_data[['head_angle','blue_zero_x','blue_zero_y']].head(n=5)\n"
   ]
  },
  {
   "source": [
    "# TO DO\n",
    "\n",
    "### Head Stim Angles\n",
    "\n",
    "Something we need to add is the relationship between the head location and stimulus location. However this requires a bit more information than we currently have, because stimulus locations are known in the real world (e.g. in measurements in cm relative to the grid) whereas LED positions are known only in terms of their position relative to the camera, in pixel values. \n",
    "\n",
    "We therefore need some form of calibration, either by mapping LED positions into the world (using some knowledge about the distance of grid features in the image), or by knowing the pixel positions of speakers. I'm going to map the centre positions of each speaker location to make the "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = stim\n",
    "\n",
    "\n",
    "join_data.to_csv( )"
   ]
  }
 ]
}